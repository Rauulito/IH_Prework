{"cells":[{"cell_type":"markdown","id":"4a98624a","metadata":{"id":"4a98624a"},"source":["# Recall Machine Learning Linear Regression\n","\n","At the end of this Lesson the studen will remember the main steps to train a model:\n","\n"," - Split dataset in train and test subsets\n"," - Standardize continuous varuables\n"," - Transform categorical variables to dummy\n"," - Train linear regression models\n"," - Train classification models\n"," - Interpret the error and accuracy metrics to validate the built models\n","\n","**You have two exercises at the end of the notebook**"]},{"cell_type":"markdown","id":"7589307d","metadata":{"id":"7589307d"},"source":["### Import data and libraries"]},{"cell_type":"code","execution_count":null,"id":"23ad14c6","metadata":{"id":"23ad14c6"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import statsmodels.api as sm\n","from sklearn.linear_model import LinearRegression\n","from sklearn import metrics\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler"]},{"cell_type":"code","execution_count":null,"id":"e2e209b3","metadata":{"scrolled":true,"id":"e2e209b3","outputId":"02ac553d-f386-455c-c1c2-0d39fff82ce5"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Species</th>\n","      <th>Weight</th>\n","      <th>Length1</th>\n","      <th>Length2</th>\n","      <th>Length3</th>\n","      <th>Height</th>\n","      <th>Width</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Bream</td>\n","      <td>242.0</td>\n","      <td>23.2</td>\n","      <td>25.4</td>\n","      <td>30.0</td>\n","      <td>11.5200</td>\n","      <td>4.0200</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Bream</td>\n","      <td>290.0</td>\n","      <td>24.0</td>\n","      <td>26.3</td>\n","      <td>31.2</td>\n","      <td>12.4800</td>\n","      <td>4.3056</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Bream</td>\n","      <td>340.0</td>\n","      <td>23.9</td>\n","      <td>26.5</td>\n","      <td>31.1</td>\n","      <td>12.3778</td>\n","      <td>4.6961</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Bream</td>\n","      <td>363.0</td>\n","      <td>26.3</td>\n","      <td>29.0</td>\n","      <td>33.5</td>\n","      <td>12.7300</td>\n","      <td>4.4555</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Bream</td>\n","      <td>430.0</td>\n","      <td>26.5</td>\n","      <td>29.0</td>\n","      <td>34.0</td>\n","      <td>12.4440</td>\n","      <td>5.1340</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>154</th>\n","      <td>Smelt</td>\n","      <td>12.2</td>\n","      <td>11.5</td>\n","      <td>12.2</td>\n","      <td>13.4</td>\n","      <td>2.0904</td>\n","      <td>1.3936</td>\n","    </tr>\n","    <tr>\n","      <th>155</th>\n","      <td>Smelt</td>\n","      <td>13.4</td>\n","      <td>11.7</td>\n","      <td>12.4</td>\n","      <td>13.5</td>\n","      <td>2.4300</td>\n","      <td>1.2690</td>\n","    </tr>\n","    <tr>\n","      <th>156</th>\n","      <td>Smelt</td>\n","      <td>12.2</td>\n","      <td>12.1</td>\n","      <td>13.0</td>\n","      <td>13.8</td>\n","      <td>2.2770</td>\n","      <td>1.2558</td>\n","    </tr>\n","    <tr>\n","      <th>157</th>\n","      <td>Smelt</td>\n","      <td>19.7</td>\n","      <td>13.2</td>\n","      <td>14.3</td>\n","      <td>15.2</td>\n","      <td>2.8728</td>\n","      <td>2.0672</td>\n","    </tr>\n","    <tr>\n","      <th>158</th>\n","      <td>Smelt</td>\n","      <td>19.9</td>\n","      <td>13.8</td>\n","      <td>15.0</td>\n","      <td>16.2</td>\n","      <td>2.9322</td>\n","      <td>1.8792</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>159 rows × 7 columns</p>\n","</div>"],"text/plain":["    Species  Weight  Length1  Length2  Length3   Height   Width\n","0     Bream   242.0     23.2     25.4     30.0  11.5200  4.0200\n","1     Bream   290.0     24.0     26.3     31.2  12.4800  4.3056\n","2     Bream   340.0     23.9     26.5     31.1  12.3778  4.6961\n","3     Bream   363.0     26.3     29.0     33.5  12.7300  4.4555\n","4     Bream   430.0     26.5     29.0     34.0  12.4440  5.1340\n","..      ...     ...      ...      ...      ...      ...     ...\n","154   Smelt    12.2     11.5     12.2     13.4   2.0904  1.3936\n","155   Smelt    13.4     11.7     12.4     13.5   2.4300  1.2690\n","156   Smelt    12.2     12.1     13.0     13.8   2.2770  1.2558\n","157   Smelt    19.7     13.2     14.3     15.2   2.8728  2.0672\n","158   Smelt    19.9     13.8     15.0     16.2   2.9322  1.8792\n","\n","[159 rows x 7 columns]"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["df = pd.read_csv('..\\data\\Fish.csv')\n","df"]},{"cell_type":"code","execution_count":null,"id":"b223673d","metadata":{"id":"b223673d","outputId":"513cc9a3-6e6c-434e-a3b9-748a0d5f6d53"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 159 entries, 0 to 158\n","Data columns (total 7 columns):\n"," #   Column   Non-Null Count  Dtype  \n","---  ------   --------------  -----  \n"," 0   Species  159 non-null    object \n"," 1   Weight   159 non-null    float64\n"," 2   Length1  159 non-null    float64\n"," 3   Length2  159 non-null    float64\n"," 4   Length3  159 non-null    float64\n"," 5   Height   159 non-null    float64\n"," 6   Width    159 non-null    float64\n","dtypes: float64(6), object(1)\n","memory usage: 8.8+ KB\n"]}],"source":["df.info()"]},{"cell_type":"markdown","id":"bc62392a","metadata":{"id":"bc62392a"},"source":["### Species variable treatment\n","\n","Species is a categorical variable, hence we need to transform it to dummies before inserting in the model\n","\n","https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html\n"]},{"cell_type":"code","execution_count":null,"id":"a419c133","metadata":{"id":"a419c133","outputId":"3003aa16-8342-4aa5-fe19-033e078afdc8"},"outputs":[{"data":{"text/plain":["Perch        56\n","Bream        35\n","Roach        20\n","Pike         17\n","Smelt        14\n","Parkki       11\n","Whitefish     6\n","Name: Species, dtype: int64"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["df.Species.value_counts()"]},{"cell_type":"markdown","id":"e0614800","metadata":{"id":"e0614800"},"source":["Firstly, let's reduce the categories to Perch, Bream and Others"]},{"cell_type":"code","execution_count":null,"id":"e973527c","metadata":{"scrolled":true,"id":"e973527c","outputId":"a8d35eb3-e3c5-4e3b-efa8-a82c9de54bf4"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Species</th>\n","      <th>Weight</th>\n","      <th>Length1</th>\n","      <th>Length2</th>\n","      <th>Length3</th>\n","      <th>Height</th>\n","      <th>Width</th>\n","      <th>fish_species</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Bream</td>\n","      <td>242.0</td>\n","      <td>23.2</td>\n","      <td>25.4</td>\n","      <td>30.0</td>\n","      <td>11.5200</td>\n","      <td>4.0200</td>\n","      <td>Bream</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Bream</td>\n","      <td>290.0</td>\n","      <td>24.0</td>\n","      <td>26.3</td>\n","      <td>31.2</td>\n","      <td>12.4800</td>\n","      <td>4.3056</td>\n","      <td>Bream</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Bream</td>\n","      <td>340.0</td>\n","      <td>23.9</td>\n","      <td>26.5</td>\n","      <td>31.1</td>\n","      <td>12.3778</td>\n","      <td>4.6961</td>\n","      <td>Bream</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Bream</td>\n","      <td>363.0</td>\n","      <td>26.3</td>\n","      <td>29.0</td>\n","      <td>33.5</td>\n","      <td>12.7300</td>\n","      <td>4.4555</td>\n","      <td>Bream</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Bream</td>\n","      <td>430.0</td>\n","      <td>26.5</td>\n","      <td>29.0</td>\n","      <td>34.0</td>\n","      <td>12.4440</td>\n","      <td>5.1340</td>\n","      <td>Bream</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>154</th>\n","      <td>Smelt</td>\n","      <td>12.2</td>\n","      <td>11.5</td>\n","      <td>12.2</td>\n","      <td>13.4</td>\n","      <td>2.0904</td>\n","      <td>1.3936</td>\n","      <td>Others</td>\n","    </tr>\n","    <tr>\n","      <th>155</th>\n","      <td>Smelt</td>\n","      <td>13.4</td>\n","      <td>11.7</td>\n","      <td>12.4</td>\n","      <td>13.5</td>\n","      <td>2.4300</td>\n","      <td>1.2690</td>\n","      <td>Others</td>\n","    </tr>\n","    <tr>\n","      <th>156</th>\n","      <td>Smelt</td>\n","      <td>12.2</td>\n","      <td>12.1</td>\n","      <td>13.0</td>\n","      <td>13.8</td>\n","      <td>2.2770</td>\n","      <td>1.2558</td>\n","      <td>Others</td>\n","    </tr>\n","    <tr>\n","      <th>157</th>\n","      <td>Smelt</td>\n","      <td>19.7</td>\n","      <td>13.2</td>\n","      <td>14.3</td>\n","      <td>15.2</td>\n","      <td>2.8728</td>\n","      <td>2.0672</td>\n","      <td>Others</td>\n","    </tr>\n","    <tr>\n","      <th>158</th>\n","      <td>Smelt</td>\n","      <td>19.9</td>\n","      <td>13.8</td>\n","      <td>15.0</td>\n","      <td>16.2</td>\n","      <td>2.9322</td>\n","      <td>1.8792</td>\n","      <td>Others</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>159 rows × 8 columns</p>\n","</div>"],"text/plain":["    Species  Weight  Length1  Length2  Length3   Height   Width fish_species\n","0     Bream   242.0     23.2     25.4     30.0  11.5200  4.0200        Bream\n","1     Bream   290.0     24.0     26.3     31.2  12.4800  4.3056        Bream\n","2     Bream   340.0     23.9     26.5     31.1  12.3778  4.6961        Bream\n","3     Bream   363.0     26.3     29.0     33.5  12.7300  4.4555        Bream\n","4     Bream   430.0     26.5     29.0     34.0  12.4440  5.1340        Bream\n","..      ...     ...      ...      ...      ...      ...     ...          ...\n","154   Smelt    12.2     11.5     12.2     13.4   2.0904  1.3936       Others\n","155   Smelt    13.4     11.7     12.4     13.5   2.4300  1.2690       Others\n","156   Smelt    12.2     12.1     13.0     13.8   2.2770  1.2558       Others\n","157   Smelt    19.7     13.2     14.3     15.2   2.8728  2.0672       Others\n","158   Smelt    19.9     13.8     15.0     16.2   2.9322  1.8792       Others\n","\n","[159 rows x 8 columns]"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["def fish_species(x):\n","    if x == 'Perch':\n","        return 'Perch'\n","    elif x == 'Bream':\n","        return 'Bream'\n","    else:\n","        return 'Others'\n","\n","df['fish_species'] = df['Species'].apply(fish_species)\n","df"]},{"cell_type":"markdown","id":"01a42172","metadata":{"id":"01a42172"},"source":["### Get dummies\n","\n","https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html\n","\n","https://stats.stackexchange.com/questions/350492/why-do-we-create-dummy-variables\n","\n","https://towardsdatascience.com/what-are-dummy-variables-and-how-to-use-them-in-a-regression-model-ee43640d573e"]},{"cell_type":"code","execution_count":null,"id":"a3a7ff6a","metadata":{"scrolled":true,"id":"a3a7ff6a"},"outputs":[],"source":["df_dum = pd.get_dummies(df.fish_species)\n","df = df.merge(df_dum, right_index = True, left_index = True, how = 'left')\n"]},{"cell_type":"code","execution_count":null,"id":"00222b8b","metadata":{"scrolled":true,"id":"00222b8b","outputId":"63739b8d-353e-44ca-ecc7-b834b9652d65"},"outputs":[{"data":{"text/plain":["Index(['Weight', 'Length1', 'Length2', 'Length3', 'Height', 'Width', 'Bream',\n","       'Others', 'Perch'],\n","      dtype='object')"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["df.drop(['Species','fish_species'], axis = 1, inplace = True)\n","df.columns"]},{"cell_type":"markdown","id":"38c2a911","metadata":{"id":"38c2a911"},"source":["### Train test split\n","\n","It is mandatory to randomly divide the dataset into two. One for training the model and the test split for validate it.\n","\n","If error metrics are low with the test split means that our model is robust\n","\n","\n","https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n"]},{"cell_type":"code","execution_count":null,"id":"02e7faad","metadata":{"id":"02e7faad"},"outputs":[],"source":["fish_train, fish_test = train_test_split(df, test_size=0.2, random_state=0)"]},{"cell_type":"code","execution_count":null,"id":"ce85406a","metadata":{"id":"ce85406a","outputId":"c6f2bb8b-d658-418b-e9d3-dc34378c8a94"},"outputs":[{"name":"stdout","output_type":"stream","text":["(127, 9)\n","(32, 9)\n"]}],"source":["print(fish_train.shape)\n","print(fish_test.shape)"]},{"cell_type":"markdown","id":"1cb0b22a","metadata":{"id":"1cb0b22a"},"source":["### Standardize the numerical variables\n","\n","Sometimes numerical variables in our dataset have very different scales, taht's to have very different values between one column and other. That can harm model accuracy.\n","\n","For solve this situation, we standardize, that's to put every continuous variable centered in 0 and with standard deviation 1\n","\n","We **first** standardize the training set, then the test set with the training set parameters\n","\n","We do not standardize the target variable\n","\n","https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler\n","\n","https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler\n","\n","\n","https://www.askpython.com/python/examples/standardize-data-in-python#:~:text=Ways%20to%20Standardize%20Data%20in%20Python%201%201.,load_iris%20...%202%202.%20Using%20StandardScaler%20%28%29%20function\n","\n"]},{"cell_type":"code","execution_count":null,"id":"556f9391","metadata":{"id":"556f9391"},"outputs":[],"source":["scale= StandardScaler()\n","\n","variables_sc = ['duration_ms', 'loudness', 'tempo']\n","\n","scale_fit = scale.fit(X_train1[variables_sc])\n","\n","X_train_sc = pd.DataFrame(scale.transform(X_train1[variables_sc]), columns = variables_sc)\n","\n","X_test_sc = pd.DataFrame(scale.transform(X_test1[variables_sc]), columns = variables_sc)\n","\n","X_train_sc.shape\n","\n","X_train1.drop(variables_sc, axis = 1, inplace = True)\n","X_train1 = X_train1.reset_index(drop = True)\n","y_train1 = y_train1.reset_index(drop=True)\n","X_train = pd.concat([X_train1, X_train_sc], axis = 1)\n","\n","X_test1.drop(variables_sc, axis = 1, inplace = True)\n","X_test1 = X_test1.reset_index(drop = True)\n","y_test1 = y_test1.reset_index(drop=True)\n","X_test = pd.concat([X_test1, X_test_sc], axis = 1)"]},{"cell_type":"code","execution_count":null,"id":"4d1580d5","metadata":{"id":"4d1580d5"},"outputs":[],"source":["scale= StandardScaler()\n","variables_sc = ['Length1', 'Length2', 'Length3', 'Height', 'Width']\n","\n","X_train = fish_train[['Length1', 'Length2', 'Length3', 'Height', 'Width', 'Bream','Others', 'Perch']]\n","y_train  = fish_train['Weight']\n","\n","X_test = fish_test[['Length1', 'Length2', 'Length3', 'Height', 'Width', 'Bream', 'Others', 'Perch']]\n","y_test  = fish_test['Weight']\n","\n","\n","scale_train = scale.fit(X_train[variables_sc])\n","\n","X_train_sc = pd.DataFrame(scale_train.transform(X_train[variables_sc]), columns = [variables_sc])\n","X_train = X_train.drop(variables_sc, axis = 1) # , inplace = True\n","X_train = X_train.reset_index(drop = True)\n","X_train = pd.concat([X_train, X_train_sc], axis = 1)\n","X_train.columns = ['Length1', 'Length2', 'Length3', 'Height', 'Width', 'Bream','Others', 'Perch']\n","y_train = y_train.reset_index(drop=True)\n","\n","X_test_sc = pd.DataFrame(scale_train.transform(X_test[variables_sc]), columns =[variables_sc])\n","X_test = X_test.drop(variables_sc, axis = 1) # , inplace = True\n","X_test = X_test.reset_index(drop = True)\n","X_test = pd.concat([X_test, X_test_sc], axis = 1)\n","X_test.columns = ['Length1', 'Length2', 'Length3', 'Height', 'Width', 'Bream','Others', 'Perch']\n","y_test = y_test.reset_index(drop=True)"]},{"cell_type":"markdown","id":"3d3a411c","metadata":{"id":"3d3a411c"},"source":["## Linear Regression\n","\n","https://medium.com/swlh/interpreting-linear-regression-through-statsmodels-summary-4796d359035a"]},{"cell_type":"code","execution_count":null,"id":"53e13378","metadata":{"scrolled":false,"id":"53e13378","outputId":"f88d3703-8f0a-4023-aabd-1f6f0bea7b64"},"outputs":[{"name":"stdout","output_type":"stream","text":["                            OLS Regression Results                            \n","==============================================================================\n","Dep. Variable:                 Weight   R-squared:                       0.895\n","Model:                            OLS   Adj. R-squared:                  0.889\n","Method:                 Least Squares   F-statistic:                     144.5\n","Date:                Tue, 12 Sep 2023   Prob (F-statistic):           4.55e-55\n","Time:                        13:06:34   Log-Likelihood:                -774.25\n","No. Observations:                 127   AIC:                             1564.\n","Df Residuals:                     119   BIC:                             1587.\n","Df Model:                           7                                         \n","Covariance Type:            nonrobust                                         \n","==============================================================================\n","                 coef    std err          t      P>|t|      [0.025      0.975]\n","------------------------------------------------------------------------------\n","const          0.0001     76.904    1.5e-06      1.000    -152.278     152.278\n","Length1        0.0001     48.957   2.16e-06      1.000     -96.939      96.939\n","Length2      2.75e-05     30.726   8.95e-07      1.000     -60.841      60.842\n","Length3    -1.756e-05     36.916  -4.76e-07      1.000     -73.097      73.097\n","Height         0.0002    468.057   4.86e-07      1.000    -926.799     926.800\n","Width          0.0003    433.742   6.45e-07      1.000    -858.852     858.853\n","Bream         -0.0005     34.887  -1.35e-05      1.000     -69.080      69.079\n","Others       4.07e-05     45.408   8.96e-07      1.000     -89.913      89.913\n","Perch       4.229e-05     46.619   9.07e-07      1.000     -92.311      92.311\n","predict        1.0000      0.268      3.738      0.000       0.470       1.530\n","==============================================================================\n","Omnibus:                       33.011   Durbin-Watson:                   1.687\n","Prob(Omnibus):                  0.000   Jarque-Bera (JB):               77.695\n","Skew:                           1.024   Prob(JB):                     1.35e-17\n","Kurtosis:                       6.239   Cond. No.                     3.63e+18\n","==============================================================================\n","\n","Notes:\n","[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n","[2] The smallest eigenvalue is 2.34e-30. This might indicate that there are\n","strong multicollinearity problems or that the design matrix is singular.\n"]}],"source":["X_train = sm.add_constant(X_train)\n","result = sm.OLS(y_train, X_train).fit()\n","\n","print(result.summary())"]},{"cell_type":"code","execution_count":null,"id":"d33fde23","metadata":{"scrolled":true,"id":"d33fde23"},"outputs":[],"source":["X_train['predict'] = result.predict(X_train)\n","\n","X_test = sm.add_constant(X_test)\n","X_test['predict'] = result.predict(X_test)"]},{"cell_type":"code","execution_count":null,"id":"216f039c","metadata":{"id":"216f039c","outputId":"d4577c56-07b4-40ee-f806-55d39082f553"},"outputs":[{"name":"stdout","output_type":"stream","text":["MAE:  79.48159685784069\n","MSE:  11556.170674549487\n","RMSE:  107.49963104378305\n","MAPE:  1.5084644534756303e-14\n","R2:  0.8947146195772226\n"]}],"source":["def mean_absolute_percentage_error(y_true, y_pred):\n","    return np.mean(np.abs((sum(y_true) - sum(y_pred)) / sum(y_true))) * 100\n","\n","print(\"MAE: \", metrics.mean_absolute_error(y_train, X_train['predict']))\n","print(\"MSE: \", metrics.mean_squared_error(y_train, X_train['predict']))\n","print(\"RMSE: \", np.sqrt(metrics.mean_squared_error(y_train, X_train['predict'])))\n","print(\"MAPE: \", mean_absolute_percentage_error(y_train, X_train['predict']))\n","print(\"R2: \", metrics.r2_score(y_train, X_train['predict']))\n"]},{"cell_type":"code","execution_count":null,"id":"e8b4c39d","metadata":{"id":"e8b4c39d","outputId":"cfba6e26-fbb3-48d8-ac05-04a4b6dc7c61"},"outputs":[{"name":"stdout","output_type":"stream","text":["MAE:  99.17774652309242\n","MSE:  26636.577496367398\n","RMSE:  163.20716129008372\n","MAPE:  3.159067095609248\n","R2:  0.8600657372046447\n"]}],"source":["print(\"MAE: \", metrics.mean_absolute_error(y_test, X_test['predict']))\n","print(\"MSE: \", metrics.mean_squared_error(y_test, X_test['predict']))\n","print(\"RMSE: \", np.sqrt(metrics.mean_squared_error(y_test, X_test['predict'])))\n","print(\"MAPE: \", mean_absolute_percentage_error(y_test, X_test['predict']))\n","print(\"R2: \", metrics.r2_score(y_test, X_test['predict']))\n"]},{"cell_type":"markdown","id":"bbbaee41","metadata":{"id":"bbbaee41"},"source":["## Exercise 1\n","\n","Response the answers in 4-5 lines each, read the links you have along this document, or in the theory notebooks, or you can also search on the internet:\n","\n"," - Which type of variables do we transform into dummies? Why do we do it?\n"," - Why is so important to divide our data into train and test datasets? Which is the purpose of doing it?\n"," - Why do we standardize some varaiables? Which type of variables do we standardize?"]},{"cell_type":"markdown","source":["**Exercise 1_1** \\\\\n","Transformamos variables categóricas en variables dummy o variables ficticias. Esto se hace para poder incluir estas variables categóricas en modelos estadísticos o algoritmos de aprendizaje automático que requieren datos numéricos como entrada."],"metadata":{"id":"5F2FrwcwR714"},"id":"5F2FrwcwR714"},{"cell_type":"markdown","source":["**Exercise 1_2** \\\\\n"," dividir los datos en conjuntos de entrenamiento y prueba es esencial para evaluar y mejorar la capacidad de generalización de un modelo de aprendizaje automático. Ayuda a evitar el sobreajuste, ajustar los hiperparámetros de manera efectiva y estimar el rendimiento real del modelo en datos no vistos, lo que es fundamental para construir modelos predictivos confiables y útiles."],"metadata":{"id":"4LBdJtd-SMKR"},"id":"4LBdJtd-SMKR"},{"cell_type":"markdown","source":["**Exercise 1_3** \\\\\n","La estandarización de datos, en ocasiones también conocida como normalización, es el proceso de ajustar o adaptar ciertas características para que los datos se asemejen a un tipo, modelo o normal común con el objetivo de que su tratamiento, acceso y uso sea más sencillo para los usuarios o personas que dispongan de ellos, mejoran la interpretabilidad y la precisión de los resultados en análisis estadísticos y modelos de aprendizaje automático. \\\\\n","Estandarizamos variables númericas"],"metadata":{"id":"dc2vIpr_TWko"},"id":"dc2vIpr_TWko"},{"cell_type":"markdown","id":"42f23018","metadata":{"id":"42f23018"},"source":["## Exercise 2\n","\n","Regarding the summary and the errors, Would you use this model to predict the weights of the fishes? Justify your answer. Comment the usefulness of the main indicators of the summary and the errors.\n"]},{"cell_type":"markdown","source":["Si usaria este modelo para predecir los pesos de los peces ya que vemos que existe una relación lineal entre  todas las variables y el peso de los peces. \\\\\n","En cuanto a los errores primero se hacen los de train y luego los de test."],"metadata":{"id":"LQSS6d-vU61y"},"id":"LQSS6d-vU61y"},{"cell_type":"code","source":[],"metadata":{"id":"wT_Na2WPVLXW"},"id":"wT_Na2WPVLXW","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}